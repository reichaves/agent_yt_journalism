import streamlit as st
from groq import Client

def render_rag_tab():
    st.header("üìå Perguntas sobre o v√≠deo")
    question = st.text_input("Digite sua pergunta sobre o conte√∫do transcrito:")
    api_key = st.session_state.get("groq_api_key", "")
    transcript = st.session_state.get("transcript", "")
    vectorstore = st.session_state.get("vectorstore", None)

    if not api_key:
        st.warning("Por favor, insira sua chave de API do Groq na aba 'Configura√ß√µes'.")
        return

    if not transcript:
        st.warning("Transcri√ß√£o n√£o encontrada. Transcreva e indexe um v√≠deo primeiro.")
        return

    if st.button("Responder"):
        with st.spinner("Consultando..."):
            try:
                response = ask_question(question, transcript, api_key)
                st.markdown("### Resposta")
                st.write(response)
            except Exception as e:
                st.error(f"Erro ao processar a pergunta: {str(e)}")

def ask_question(question: str, transcript: str, groq_api_key: str) -> str:
    client = Client(api_key=groq_api_key)

    system_prompt = (
        "Voc√™ √© um assistente especializado em v√≠deos que responde perguntas com base "
        "em transcri√ß√µes. Responda apenas com informa√ß√µes encontradas no contexto fornecido. "
        "Se a resposta n√£o estiver no contexto, diga que n√£o √© poss√≠vel responder com base na transcri√ß√£o."
    )

    user_prompt = f"""
    Responda √† seguinte pergunta com base no conte√∫do abaixo:

    Transcri√ß√£o do v√≠deo:
    {transcript}

    Pergunta: {question}
    """

    chat_response = client.chat.completions.create(
        model="deepseek-r1-distill-llama-70b",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=0.2,
        max_tokens=2048
    )

    return chat_response.choices[0].message.content
